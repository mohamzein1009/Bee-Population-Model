{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9529d12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T08:42:15.781822Z",
     "start_time": "2023-08-29T08:42:12.125993Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import pyproj\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa44f8",
   "metadata": {},
   "source": [
    "# InitialData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2cda49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T09:57:43.910974Z",
     "start_time": "2023-08-29T09:57:42.119828Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp/ipykernel_20800/2863187422.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  post = pd.read_csv('Extra Data/open_postcode_elevation.csv', header=None)\n"
     ]
    }
   ],
   "source": [
    "#raw data\n",
    "\n",
    "post = pd.read_csv('Extra Data/open_postcode_elevation.csv', header=None)\n",
    "orch = pd.read_csv('Extra Data/Traditional_Orchards_HAP.csv')\n",
    "\n",
    "bees = pd.read_csv('Final Data/coords_species.csv') \n",
    "bees = bees.rename(columns = {'Lat':'lat', 'Long':'lon'})\n",
    "\n",
    "shortbees = pd.read_csv('Final Data/shortbees.csv')\n",
    "shortbees = shortbees.rename(columns = {'Lat':'lat', 'Long':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d5e698d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:13:16.754608Z",
     "start_time": "2023-07-14T14:13:16.728029Z"
    }
   },
   "outputs": [],
   "source": [
    "coords = shortbees[['lat', 'lon']].drop_duplicates().reset_index()[['lat', 'lon']] #list of unique coordinates to create \"dictionary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7da853",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6d3ca2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:23:13.583121Z",
     "start_time": "2023-07-14T13:23:13.570156Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def coordPairs(df): # for all coordinates in the bees dataset, find all the nearest coordinates from the new datasets\n",
    "    indexs = [] #keep track of nearest coordinate\n",
    "    \n",
    "    #\"dictionary\"\n",
    "    for i, j in coords.iterrows(): \n",
    "        min_dist = 100000 #inital value\n",
    "        c1 = (j['lat'], j['lon']) \n",
    "        \n",
    "        for m, n in df[['lat', 'lon']].iterrows():\n",
    "            c2 = (n['lat'], n['lon'])\n",
    "            x = distance(c1, c2) #calculate distance\n",
    "            \n",
    "            if x == 0: #stop if theres an exact match\n",
    "                index = m\n",
    "                break\n",
    "                \n",
    "            if x <= min_dist: #find closest coords\n",
    "                min_dist = x\n",
    "                index = m\n",
    "                \n",
    "        indexs.append(index)\n",
    "    \n",
    "    # \"dictionary\"\n",
    "    close = df.iloc[indexs].reset_index()\n",
    "    pairs = pd.concat([coords, close[['index', 'lat', 'lon']]], axis = 1)\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76b023db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:23:14.144058Z",
     "start_time": "2023-07-14T13:23:14.123548Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def fill(df, col, coord_pairs, op = None): #fill the bees dataset with the extra data using the nearest coordinates\n",
    "\n",
    "    #filling data using dictionary\n",
    "    final_indexs = []\n",
    "    \n",
    "    for i, j in shortbees.iterrows():\n",
    "        lat = j['lat']\n",
    "        lon = j['lon']\n",
    "        \n",
    "        q = coord_pairs.query('lat == @lat and lon == @lon')['index'].iloc[0]\n",
    "\n",
    "        final_indexs.append(q)\n",
    "        \n",
    "    final_df = df.iloc[final_indexs].reset_index()\n",
    "    \n",
    "    if op == 'bin': #convert columns to binary indicators\n",
    "        final_df[col] = np.where(final_df[col] >= 1, 1, 0)\n",
    "        \n",
    "    newdf = pd.concat([shortbees, final_df[col]], axis = 1)\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bf58924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:23:14.704414Z",
     "start_time": "2023-07-14T13:23:14.694437Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def shrink(data, op = None): #round coordinates and group points with the same coordinates\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    df[['lat', 'lon']] = df[['lat', 'lon']].applymap(lambda x: round(x, 3 - int(np.floor(np.log10(abs(x))))))\n",
    "    \n",
    "    if op == 'av':\n",
    "        grouped = df.groupby(['lat', 'lon']).mean().reset_index()\n",
    "        \n",
    "    else:\n",
    "        grouped = df.groupby(['lat', 'lon']).sum().reset_index()\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d23f935a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:23:18.142167Z",
     "start_time": "2023-07-14T13:23:18.131031Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def distance(z1, z2): #calculate euclidian distance\n",
    "    x = (z1[0] - z2[0])**2 + (z1[1] - z2[1])**2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4485885",
   "metadata": {},
   "source": [
    "# Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357c1bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T01:38:01.840154Z",
     "start_time": "2023-07-10T01:37:58.970824Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#postcode\n",
    "post = post[[1, 2]]\n",
    "post.columns = ['postcode', 'elevation']\n",
    "\n",
    "post.to_csv('postcodes.csv')\n",
    "\n",
    "shortened = post[::5] #use every 5th row as observation becasue the dataset is too large\n",
    "\n",
    "shortened.to_csv('short.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ab895ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:13:28.420466Z",
     "start_time": "2023-07-14T14:13:28.134238Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#elevation\n",
    "elev = pd.read_csv('elev.csv') #shortened was inputted into a program that took postcodes and returned their coordinates and the output is elev\n",
    "\n",
    "elev = elev.drop(columns=['postcode'])\n",
    "\n",
    "elev = elev.rename(columns = {'Latitude':'lat', 'Longitude':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfc71af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:13:37.595325Z",
     "start_time": "2023-07-14T14:13:35.526720Z"
    }
   },
   "outputs": [],
   "source": [
    "elev_shrink = shrink(elev, 'av')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "188ac4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_pairs = coordPairs(elev_shrink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "506b277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_pairs.to_csv('elev_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfa4fc19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:13:51.237125Z",
     "start_time": "2023-07-14T14:13:51.225117Z"
    }
   },
   "outputs": [],
   "source": [
    "elev_pairs = pd.read_csv('elev_pairs.csv')\n",
    "\n",
    "elev_pairs = elev_pairs.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4831af21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:14:10.350324Z",
     "start_time": "2023-07-14T14:13:52.626745Z"
    }
   },
   "outputs": [],
   "source": [
    "shortbees = fill(elev_shrink, ['elevation'], elev_pairs.iloc[:, 0:3]) #adding elevation data too the bee dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317681ec",
   "metadata": {},
   "source": [
    "# Orchard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862ce4de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T09:57:49.890764Z",
     "start_time": "2023-08-29T09:57:48.306000Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#orch\n",
    "#cleaning inconsistent values in the dataset\n",
    "\n",
    "orch = orch.drop(columns=['OBJECTID', 'Prihabtxt', 'Habdefver', 'S5OSMM', 'NonTOcode', 'OS_Epoch2', 'Area_Ha', \n",
    "                          'Shape_Length', 'Shape__Area', 'Shape__Length', 'S1GroundSv', 'S1Status', 'S1Planting',\n",
    "                          'S2Status', 'S2Planting', 'S3Status', 'S3Planting', 'S4Status', 'S5OSMM', 'Stewardshp', \n",
    "                          'S2OrchQues'])\n",
    "\n",
    "orch['lon'], orch['lat'] = convert_lonlat(orch['Easting'], orch['Northing'])\n",
    "\n",
    "orch['S3Habid'] = orch['S3Habid'].replace(['Primary', 'Secondary', 'Tertiary', ' ', 'Other'], \n",
    "                                          [1, 2, 3, np.nan, np.nan])\n",
    "\n",
    "orch['S4Habid'] = orch['S4Habid'].replace(['Primary', 'Secondary', 'Tertiary', 'Other', ' '], \n",
    "                                          [1, 2, 3, np.nan, np.nan])\n",
    "\n",
    "orch['InterQual'] = orch['InterQual'].replace(['3 (Medium)', 'Medium (3)', '3'], 3)\n",
    "orch['InterQual'] = orch['InterQual'].replace(['1 (High)', 'High (1)', '1', '1 (high)'], 1)\n",
    "orch['InterQual'] = orch['InterQual'].replace(['2 (Medium)', '2'], 2)\n",
    "orch['InterQual'] = orch['InterQual'].replace(['4 (Medium)'], 4)\n",
    "orch['InterQual'] = orch['InterQual'].replace([' '], np.nan)\n",
    "\n",
    "orch['PriDet'] = orch['PriDet'].replace(['Definitely is Traditional Orchard priority habitat', \n",
    "                                        'Priority Traditional Orchard habitat may be present but evidence is either insufficient to determine presence confidently or is in the oldest allowable category',\n",
    "                                        'Probably Traditional Orchard priority habitat but some uncertainty due to age of data source'], [3,1,2])\n",
    "\n",
    "orch['HabCon'] = orch['HabCon'].replace(['Excellent', 'Good', 'Poor', ' '], [3, 2, 1, np.nan])\n",
    "\n",
    "orch['S1Habid'] = orch['S1Habid'].replace(['Primary', 'Secondary', 'SecondaryS', ' ', '25/09/2008'], \n",
    "                                          [1, 2, 2, np.nan, np.nan])\n",
    "\n",
    "orch['S2Habid'] = orch['S2Habid'].replace(['Primary', 'Secondary', '-', ' ', '26/02/2010', '06/09/2012'], \n",
    "                                          [1, 2, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "orch['S2Habid'] = orch['S2Habid'].replace(['Primary', 'Secondary', '-', ' ', '26/02/2010', '06/09/2012'], \n",
    "                                          [1, 2, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "orch['S3Boundary'] = orch['S3Boundary'].replace(['Primary', 'Secondary', ' ', '01/06/2009', '12/05/2006'], \n",
    "                                          [1, 2, np.nan, np.nan, np.nan])\n",
    "\n",
    "orch['S4Boundary'] = orch['S4Boundary'].replace(['Primary', 'Secondary', 'Tertiary', 'SecondaryS', 'Tertiart'], \n",
    "                                          [1, 2, 3, 2, 3])\n",
    "\n",
    "orch['S5Boundary'] = orch['S5Boundary'].replace(['Primary', 'Secondary', 'Tertiary', '18/09/2004', ' ', 'Boundary u'], \n",
    "                                          [1, 2, 3, np.nan, np.nan, np.nan])\n",
    "\n",
    "for i in ['Apple', 'Pear', 'Plum', 'Cherry', 'Damson', 'VetTrees10', \n",
    "          'Vet_11_30', 'Vet_31_100', 'Vet_100', 'YFT10', 'YFT_11_30', 'YFT_31_100', \n",
    "          'YFT_100', 'Grazed', 'Mown', \n",
    "          'Unmanaged', 'Sheep', 'Cattle', 'Equine', 'Pigs', 'Fowl', 'GrazingDmg', \n",
    "          'BranchHols', 'Loose_bark', 'Cavities', 'DW_Canopy', 'DW_Floor', 'WaterPools', \n",
    "          'Crevices', 'Sap_runs', 'FungalFrts', 'AerialRts', 'Hedgerows', 'Ponds', 'VetTrees', 'RoughAreas']:\n",
    "    orch[i] = orch[i].replace(['1', 'y', 'Y'], 1)\n",
    "    orch[i] = orch[i].replace(['?', '3', '6', '2', 'S', 's', 'D'], np.nan)\n",
    "    orch[i] = orch[i].replace([' ', 'N', '0'], 0)\n",
    "\n",
    "#features kept and also dropped any rows that have any missing values\n",
    "orch = orch[['Apple', 'Pear', 'Plum', 'Cherry', 'Damson', 'VetTrees10', \n",
    "             'Vet_11_30', 'Vet_31_100', 'Vet_100', 'YFT10', 'YFT_11_30', \n",
    "             'YFT_31_100', 'YFT_100', 'Grazed', 'Mown', 'Unmanaged', \n",
    "             'Sheep', 'Cattle', 'Equine', 'Pigs', 'Fowl', 'GrazingDmg', \n",
    "             'BranchHols', 'Loose_bark', 'Cavities', 'DW_Canopy', 'DW_Floor', \n",
    "             'WaterPools', 'Crevices', 'Sap_runs', 'FungalFrts', 'AerialRts', \n",
    "             'Hedgerows', 'Ponds', 'VetTrees', 'RoughAreas', 'lat', 'lon']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fa10853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:24:22.280904Z",
     "start_time": "2023-07-14T13:24:22.008149Z"
    }
   },
   "outputs": [],
   "source": [
    "orch_shrink = shrink(orch, 'av')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2448c459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:52:42.544788Z",
     "start_time": "2023-07-14T13:25:47.663779Z"
    }
   },
   "outputs": [],
   "source": [
    "orch_pairs = coordPairs(orch_shrink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d3da606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T13:52:42.620728Z",
     "start_time": "2023-07-14T13:52:42.555922Z"
    }
   },
   "outputs": [],
   "source": [
    "orch_pairs.to_csv('orch_pairs_av.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec62f4b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:08:55.089256Z",
     "start_time": "2023-07-14T14:08:55.071300Z"
    }
   },
   "outputs": [],
   "source": [
    "orch_pairs = pd.read_csv('orch_pairs.csv')\n",
    "\n",
    "orch_pairs = orch_pairs.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a9b0889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:15:00.806808Z",
     "start_time": "2023-07-14T14:14:42.077292Z"
    }
   },
   "outputs": [],
   "source": [
    "shortbees = fill(orch_shrink, orch_shrink.columns[2:], orch_pairs.iloc[:, 0:3]) #adding the new data to the bee dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8037953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:15:01.184590Z",
     "start_time": "2023-07-14T14:15:00.809199Z"
    }
   },
   "outputs": [],
   "source": [
    "shortbees.to_csv('final_bees_av.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
